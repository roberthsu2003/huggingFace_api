# LLM å¤§æ¨¡å‹å°ˆæ¡ˆå ±å‘Š

æœ¬æ–‡ä»¶æ—¨åœ¨èªªæ˜å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å°ˆæ¡ˆçš„çµæ§‹ã€ç›®æ¨™èˆ‡å¯¦ä½œç´°ç¯€ã€‚å°ˆæ¡ˆæ¶µè“‹äº†å¾è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ ¸å¿ƒæŠ€è¡“åˆ°é€šç”¨èªè¨€æ¨¡å‹æ‡‰ç”¨çš„å¤šå€‹é¢å‘ï¼Œä¸¦é‡å°ç‰¹å®šå ´æ™¯é€²è¡Œäº†å®¢è£½åŒ–é–‹ç™¼ã€‚

## å°ˆæ¡ˆçµæ§‹

- **source_for_tw/**: å°ç£ç‰¹å®šè³‡æºã€‚
- **source_hugging_face/**: Hugging Face ç›¸é—œè³‡æºèˆ‡æ¨¡å‹è©•æ¸¬ã€‚
- **æ³•å‹™éƒ¨/**: é‡å°æ³•å‹™éƒ¨æ‡‰ç”¨çš„å®¢è£½åŒ–æ¨¡å‹èˆ‡è§£æ±ºæ–¹æ¡ˆã€‚
- **è‡ªç„¶èªè¨€/**: è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ ¸å¿ƒæŠ€è¡“çš„å¯¦ä½œèˆ‡å±•ç¤ºã€‚
- **é€šç”¨èªè¨€/**: é€šç”¨èªè¨€æ¨¡å‹çš„æ‡‰ç”¨èˆ‡ä»‹é¢ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šè‡ªç„¶èªè¨€è™•ç†æ ¸å¿ƒæŠ€è¡“

è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯æœ¬å°ˆæ¡ˆçš„åŸºçŸ³ï¼Œæˆ‘å€‘å¯¦ä½œäº†å¤šé …é—œéµæŠ€è¡“ï¼ŒåŒ…å«å‘½åå¯¦é«”è¾¨è­˜ï¼ˆNERï¼‰ã€æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ç­‰ã€‚

### 1.1 å‘½åå¯¦é«”è¾¨è­˜ (Named Entity Recognition, NER)

å‘½åå¯¦é«”è¾¨è­˜æ—¨åœ¨å¾æ–‡æœ¬ä¸­è­˜åˆ¥å‡ºå…·æœ‰ç‰¹å®šæ„ç¾©çš„å¯¦é«”ï¼Œä¾‹å¦‚äººåã€åœ°åã€çµ„ç¹”åç­‰ã€‚æˆ‘å€‘ä½¿ç”¨äº† `ckiplab/bert-base-chinese-ner` å’Œè‡ªè¡Œå¾®èª¿çš„ `roberthsu2003/models_for_ner` æ¨¡å‹é€²è¡Œå¯¦ä½œã€‚

#### æ¨¡å‹ä»‹ç´¹

- **æ¨¡å‹ (ä¸­å¤®ç ”ç©¶é™¢)**: `ckiplab/bert-base-chinese-ner`
- **æ¨¡å‹ (roberthsu2003)**: `roberthsu2003/models_for_ner`

#### ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼šä½¿ç”¨ `ckiplab/bert-base-chinese-ner`

```python
from transformers import pipeline
import numpy as np

ner_pipe = pipeline("token-classification", model='ckiplab/bert-base-chinese-ner', aggregation_strategy="simple")

inputs = "å‚…é”ä»ä»Šå°‡åŸ·è¡Œå®‰æ¨‚æ­»ï¼Œå»çªç„¶çˆ†å‡ºè‡ªå·±20å¹´å‰é­ç·¯ä¾†é«”è‚²å°å°æ®ºï¼Œä»–ä¸æ‡‚è‡ªå·±å“ªè£¡å¾—ç½ªåˆ°é›»è¦–å°ã€‚"
res = ner_pipe(inputs)

grouped_entities = []
current_group = None

for entity in res:
    if current_group is None:
        current_group = {
            'entity_group': entity['entity_group'],
            'word': entity['word'].replace(' ', ''),  # Remove spaces here
            'start': entity['start'],
            'end': entity['end'],
            'scores': [entity['score'].item()]
        }
    elif entity['entity_group'] == current_group['entity_group'] and entity['start'] == current_group['end']:
        current_group['word'] += entity['word'].replace(' ', '')  # Remove spaces here
        current_group['end'] = entity['end']
        current_group['scores'].append(entity['score'].item())
    else:
        grouped_entities.append(current_group)
        current_group = {
            'entity_group': entity['entity_group'],
            'word': entity['word'].replace(' ', ''),  # Remove spaces here
            'start': entity['start'],
            'end': entity['end'],
            'scores': [entity['score'].item()]
        }

if current_group:
    grouped_entities.append(current_group)

for entity in grouped_entities:
    print(f"Entity: {entity['word']}, Type: {entity['entity_group']}, Scores: {entity['scores']}")
```

### 1.2 æª¢ç´¢å¢å¼·ç”Ÿæˆ (Retrieval-Augmented Generation, RAG)

RAG æŠ€è¡“çµåˆäº†æª¢ç´¢ç³»çµ±èˆ‡ç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¾å¤§é‡æ–‡æœ¬ä¸­å°‹æ‰¾ç›¸é—œè³‡è¨Šï¼Œä¸¦åŸºæ–¼é€™äº›è³‡è¨Šç”Ÿæˆæ›´æº–ç¢ºã€æ›´è±å¯Œçš„å›ç­”ã€‚æˆ‘å€‘ä½¿ç”¨ Faiss å‘é‡è³‡æ–™åº«å’Œ Gradio å»ºç«‹äº†ä¸€å€‹å•ç­”ç³»çµ±ä»‹é¢ã€‚

#### ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼šFaiss èˆ‡ Gradio æ•´åˆ

```python
import gradio as gr
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
import torch
import pandas as pd

# è¼‰å…¥æ¨¡å‹å’Œtokenizer
tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')
model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# è¼‰å…¥dataset
dataset = load_dataset('roberthsu2003/data_for_RAG', split='train')
dataset = dataset.map(lambda x: {
    'text': 'passage:' + x['question'] + '\n' + x['answering']
})

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt", max_length=512
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    with torch.no_grad():
        model_output = model(**encoded_input)
        embeddings = model_output.last_hidden_state[:, 0, :]
    return embeddings

# å»ºç«‹embedding dataset
embedding_dataset = dataset.map(
    lambda x: {"embeddings": get_embeddings(x['text']).detach().cpu().numpy()[0]}
)
embedding_dataset.add_faiss_index(column="embeddings")

def search_similar_questions(question):
    # è™•ç†è¼¸å…¥å•é¡Œ
    question = "query:" + question
    question_embedding = get_embeddings([question]).cpu().detach().numpy()
    
    # æœå°‹ç›¸ä¼¼å•é¡Œ
    scores, samples = embedding_dataset.get_nearest_examples(
        "embeddings", question_embedding, k=3
    )
    
    # æ ¼å¼åŒ–è¼¸å‡ºçµæœ
    result = ""
    for score, q, a in zip(scores, samples['question'], samples['answering']):
        result += f"å•é¡Œ: {q}\n"
        result += f"ç­”æ¡ˆ: {a}\n"
        result += f"ç›¸ä¼¼åº¦åˆ†æ•¸: {score:.4f}\n"
        result += "="*50 + "\n\n"
    
    return result

# å»ºç«‹Gradioä»‹é¢
iface = gr.Interface(
    fn=search_similar_questions,
    inputs=gr.Textbox(lines=2, placeholder="è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."),
    outputs=gr.Textbox(lines=10),
    title="Tesla Model 3å•ç­”ç³»çµ±",
    description="è¼¸å…¥å•é¡Œï¼Œç³»çµ±æœƒæœå°‹æœ€ç›¸é—œçš„ç­”æ¡ˆ"
)

if __name__ == "__main__":
    iface.launch()
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šé€šç”¨èªè¨€æ¨¡å‹æ‡‰ç”¨

æœ¬éƒ¨åˆ†å°ˆæ³¨æ–¼é€šç”¨å¤§å‹èªè¨€æ¨¡å‹çš„æ‡‰ç”¨ï¼ŒåŒ…æ‹¬ Meta çš„ Llama ç³»åˆ—æ¨¡å‹ã€Ollama æ¡†æ¶ä»¥åŠ OpenWebUI çš„å®¢è£½åŒ–ã€‚

### 2.1 Llama ç³»åˆ—æ¨¡å‹æ‡‰ç”¨

æˆ‘å€‘ä½¿ç”¨äº† `meta-llama/Llama-3.2-3B-Instruct` æ¨¡å‹é€²è¡Œå¯¦é©—ã€‚æ­¤æ¨¡å‹éœ€è¦åœ¨ Hugging Face å¹³å°ç”³è«‹æ¬Šé™ï¼Œä¸¦å»ºè­°åœ¨å…·å‚™è¶³å¤ è¨˜æ†¶é«”ï¼ˆè¶…é 12GBï¼‰åŠ GPU çš„ç’°å¢ƒï¼ˆå¦‚ Google Colabï¼‰ä¸­åŸ·è¡Œã€‚

#### ä½¿ç”¨æµç¨‹
1. æ–¼ Hugging Face å¹³å°ç”³è«‹ `meta-llama/Llama-3.2-3B-Instruct` æ¨¡å‹å­˜å–æ¬Šé™ã€‚
2. å–å¾—ä¸¦è¨­å®š Hugging Face Access Tokenã€‚
3. åœ¨ Colab ç­‰ç’°å¢ƒä¸­è¨­å®šç’°å¢ƒè®Šæ•¸æˆ– Secretï¼Œä»¥ä¿è­· Tokenã€‚
4. è¼‰å…¥æ¨¡å‹ä¸¦é€²è¡Œæ¨è«–ã€‚

### 2.2 Ollama æ¡†æ¶æ‡‰ç”¨

Ollama æ˜¯ä¸€å€‹è¼•é‡ç´šçš„æ¡†æ¶ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åœ¨æœ¬æ©Ÿç«¯é‹è¡Œå¤§å‹èªè¨€æ¨¡å‹ã€‚æˆ‘å€‘å»ºç«‹äº†ä¸€å€‹ Gradio ä»‹é¢ï¼Œé€é API å‘¼å«åœ¨æœ¬æ©Ÿé‹è¡Œçš„ Llama 3.2 æ¨¡å‹ã€‚

#### ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼šOllama Gradio ä»‹é¢

```python
import requests
import json
import gradio as gr

# å‘¼å« Ollama çš„å‡½å¼
def ask_ollama(prompt):
    #url = "http://localhost:11344/api/generate"
    url = "http://host.docker.internal:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": "llama3.2:3b",
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            return response.json()['response']
        else:
            return f"âš ï¸ Error: {response.status_code} - {response.text}"
    except Exception as e:
        return f"âŒ Exception: {str(e)}"

# å»ºç«‹ Gradio ä»‹é¢
iface = gr.Interface(
    fn=ask_ollama,
    inputs=gr.Textbox(label="è¼¸å…¥ä½ çš„å•é¡Œ"),
    outputs=gr.Textbox(label="æ¨¡å‹çš„å›ç­”"),
    title="ğŸ¦™ Ollama èŠå¤©ä»‹é¢ (LLaMA3)",
    description="è¼¸å…¥æ–‡å­—ï¼Œè®“æœ¬æ©Ÿçš„ LLaMA3 æ¨¡å‹å›ç­”ä½ "
)

# å•Ÿå‹•ä»‹é¢
iface.launch()
```

### 2.3 OpenWebUI å·¥å…·ä¼ºæœå™¨

ç‚ºäº†æ“´å…… OpenWebUI çš„åŠŸèƒ½ï¼Œæˆ‘å€‘å»ºç«‹äº†è‡ªå®šç¾©çš„å·¥å…·ä¼ºæœå™¨ã€‚æ­¤ä¼ºæœå™¨å¯ä»¥è®“ LLM åŸ·è¡Œç‰¹å®šçš„å¾Œç«¯ä»»å‹™ã€‚

#### ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼šMy Tool Server (main.py)

ç”±æ–¼ `main.py` å…§å®¹ç‚ºç©ºï¼Œæ­¤è™•åƒ…æä¾›æ¦‚å¿µæ€§èªªæ˜ã€‚ä¸€å€‹åŸºæœ¬çš„å·¥å…·ä¼ºæœå™¨é€šå¸¸æœƒä½¿ç”¨ FastAPI æˆ– Flask æ¡†æ¶ï¼Œå®šç¾©æ•¸å€‹ API ç«¯é»ã€‚æ¯å€‹ç«¯é»å°æ‡‰ä¸€å€‹å·¥å…·ï¼Œæ¥æ”¶ LLM å‚³ä¾†çš„åƒæ•¸ï¼ŒåŸ·è¡Œç‰¹å®šåŠŸèƒ½ï¼ˆå¦‚è³‡æ–™æŸ¥è©¢ã€æª”æ¡ˆæ“ä½œç­‰ï¼‰ï¼Œä¸¦å°‡çµæœå›å‚³ã€‚

```python
# æ¦‚å¿µæ€§ç¯„ä¾‹ (FastAPI)
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class ToolInput(BaseModel):
    parameter: str

@app.post("/my_tool")
async def use_my_tool(tool_input: ToolInput):
    try:
        # åœ¨æ­¤åŸ·è¡Œå·¥å…·çš„é‚è¼¯
        result = f"å·¥å…·å·²æˆåŠŸåŸ·è¡Œï¼Œåƒæ•¸ç‚º: {tool_input.parameter}"
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# åŸ·è¡Œä¼ºæœå™¨: uvicorn main:app --reload
```
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d099b5d5",
   "metadata": {},
   "source": [
    "- 使用dataset(roberthsu2003/data_for_RAG)\n",
    "- 使用模型(使用Microsoft開源的intfloat/multilingual-e5-large)\n",
    "- 模型大約要2.24GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "\n",
    "class DocumentSearch:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "        self.model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def get_embedding(self, text, is_query=False):\n",
    "        #增加前綴文字\n",
    "        if is_query:\n",
    "            text = f\"query:{text}\"\n",
    "        else:\n",
    "            text = f\"passage:{text}\"\n",
    "        \n",
    "        #編碼文本\n",
    "        inputs = self.tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "\n",
    "        #獲取嵌入向量\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            #如果依據測試檢索能力的寫法是\n",
    "            #embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            #但這種方法會比較好,看下面的說明1,2\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :] #使用[CLS]token的輸出#[CLS]標記通常被用來表示整個句子的語義\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings.cpu().numpy()[0] #請看說明3\n",
    "\n",
    "    def create_document_embeddings(self, df):\n",
    "        \"\"\"為輸入的dataFrame建立嵌入向量\"\"\"\n",
    "        embeddings = []\n",
    "        for _, row in df.iterrows():\n",
    "            #組合Title和Text欄位\n",
    "            text = f\"Title:{row['Title']}\\nText:{row['Text']}\"\n",
    "            embedding = self.get_embedding(text, is_query=False)\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings\n",
    "    \n",
    "    def find_best_passage(self, query, df):\n",
    "        \"\"\"查載最相關的文本\"\"\"\n",
    "        query_embedding = self.get_embedding(query, is_query=True)\n",
    "\n",
    "        #計算相似度\n",
    "        similarities = np.dot(np.stack(df['Embeddings'].values), query_embedding)#請看說明4\n",
    "\n",
    "        #傳回最相關的文檔\n",
    "        best_idx = np.argmax(similarities)\n",
    "        return df.iloc[best_idx]['Text'], similarities[best_idx]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a14649cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:如何控制氣候?\n",
      "最相關的文字是(相似度:0.885):\n",
      "您的 Googlecar 配備氣候控制系統，可讓您調節車內的溫度和氣流。若要操作氣候控制系統，請使用中央控制台上的按鈕和旋鈕。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #初始化文檔搜尋\n",
    "    doc_search = DocumentSearch()\n",
    "\n",
    "    # 準備數據\n",
    "    documents = [\n",
    "        {\n",
    "            \"Title\": \"操作氣候控制系統\",\n",
    "            \"Text\": \"您的 Googlecar 配備氣候控制系統，可讓您調節車內的溫度和氣流。若要操作氣候控制系統，請使用中央控制台上的按鈕和旋鈕。\"\n",
    "        },\n",
    "        {\n",
    "            \"Title\": \"觸控螢幕\",\n",
    "            \"Text\": \"您的 Googlecar 擁有大型觸控螢幕顯示屏，可使用各種功能，包括導航、娛樂和氣候控制。\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 建立DataFrame\n",
    "    df = pd.DataFrame(documents)\n",
    "\n",
    "    # 生成嵌入向量\n",
    "    df['Embeddings'] = doc_search.create_document_embeddings(df)\n",
    "\n",
    "    # 測試查詢\n",
    "    query = \"如何控制氣候?\"\n",
    "\n",
    "    best_text, similarity = doc_search.find_best_passage(query, df)\n",
    "    print(f\"Query:{query}\")\n",
    "    print(f\"最相關的文字是(相似度:{similarity:.3f}):\")\n",
    "    print(best_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7747c7",
   "metadata": {},
   "source": [
    "## 說明1\n",
    "\n",
    "1. `embeddings = outputs.last_hidden_state[:, 0, :]`\n",
    "- 這行在提取BERT類模型的句子表示向量：\n",
    "  - `last_hidden_state` 的形狀是 `[batch_size, sequence_length, hidden_size]`\n",
    "  - `[:, 0, :]` 這個切片操作代表：\n",
    "    - 第一個 `:` : 選取所有batch\n",
    "    - `0` : 只取第一個token位置(即[CLS]標記)\n",
    "    - 最後的 `:` : 選取所有hidden維度\n",
    "  - [CLS]標記通常被用來表示整個句子的語義\n",
    "\n",
    "2. `embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)`\n",
    "- 這行在進行L2正規化：\n",
    "  - `p=2` : 使用L2範數(歐幾里得範數)\n",
    "  - `dim=1` : 在hidden維度上進行正規化\n",
    "  - 正規化後的向量長度會變為1，這有助於：\n",
    "    - 計算向量間的相似度\n",
    "    - 減少向量大小的影響\n",
    "    - 提高模型的穩定性\n",
    "\n",
    "簡單來說，這兩行程式碼的目的是：\n",
    "1. 獲取能夠代表整個輸入文本意義的向量表示\n",
    "2. 對向量進行標準化處理，使其更適合後續的相似度計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16037b",
   "metadata": {},
   "source": [
    "## 說明2\n",
    "這兩種取得句向量(sentence embeddings)的方法確實有所不同：\n",
    "\n",
    "### 您先前的方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outputs.last_hidden_state[:, 0, :] #使用[CLS]token的輸出\n",
    "embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88af6b4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 目前的方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fae2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd625d0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "主要差異在於：\n",
    "\n",
    "1. **向量提取方式**\n",
    "   - 先前方法：只使用 `[CLS]` token 的隱藏狀態作為整個句子的表示\n",
    "   - 目前方法：對所有 tokens 的隱藏狀態取平均值\n",
    "\n",
    "2. **正規化處理**\n",
    "   - 先前方法：有進行 L2 正規化\n",
    "   - 目前方法：沒有進行正規化\n",
    "\n",
    "3. **理論基礎**\n",
    "   - `[CLS]` token：BERT 類模型特別設計用來捕捉整個句子的語義信息\n",
    "   - 平均池化：考慮了所有 tokens 的貢獻，可能包含更多局部細節\n",
    "\n",
    "### 建議改進\n",
    "建議您使用先前的方法，因為：\n",
    "1. `[CLS]` token 是模型專門設計來表示整個句子的\n",
    "2. L2 正規化有助於提高相似度計算的穩定性\n",
    "3. 這也是 E5 模型原始論文中推薦的方法\n",
    "\n",
    "可以這樣修改當前代碼：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d238dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取句子嵌入\n",
    "sentence_embeddings = outputs.last_hidden_state[:, 0, :]  # 使用[CLS]token\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88665ce3",
   "metadata": {},
   "source": [
    "## 說明3\n",
    "讓我解釋 `embeddings.cpu().numpy()[0]` 這行代碼的每個部分：\n",
    "\n",
    "1. **embeddings**: \n",
    "   - 這是一個在 GPU 或 CPU 上的 PyTorch tensor\n",
    "   - 包含了經過模型處理後的向量表示\n",
    "\n",
    "2. **.cpu()**:\n",
    "   - 將 tensor 從 GPU 移動到 CPU\n",
    "   - 如果原本就在 CPU 上，這個操作不會有影響\n",
    "   - 這個步驟是必要的，因為 numpy 只能處理 CPU 上的數據\n",
    "\n",
    "3. **.numpy()**:\n",
    "   - 將 PyTorch tensor 轉換為 NumPy array\n",
    "   - NumPy array 在許多數據處理任務中更容易使用\n",
    "\n",
    "4. **[0]**:\n",
    "   - 選取第一個（也是唯一一個）樣本的嵌入向量\n",
    "   - 因為模型輸出的形狀是 `[1, embedding_dim]`（批次大小為1）\n",
    "   - 這個操作移除了第一個維度，得到一個一維數組\n",
    "\n",
    "完整流程：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb6d3c",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "PyTorch Tensor (GPU/CPU) \n",
    "→ CPU Tensor \n",
    "→ NumPy Array \n",
    "→ 一維向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee970b36",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "這個轉換是必要的，因為：\n",
    "- NumPy 格式更適合後續的數值計算\n",
    "- 移除批次維度使向量更容易處理\n",
    "- 確保數據在 CPU 上，可以進行更多標準的數據操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894b29",
   "metadata": {},
   "source": [
    "## 說明4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.dot(np.stack(df['Embeddings'].values), query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc513e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 程式碼拆解說明：\n",
    "\n",
    "1. **`df['Embeddings'].values`**\n",
    "   - 取得 DataFrame 中 'Embeddings' 欄位的所有向量值\n",
    "   - 每個向量代表一個文件的嵌入表示\n",
    "\n",
    "2. **`np.stack()`**\n",
    "   - 將多個嵌入向量堆疊成一個二維數組\n",
    "   - 結果形狀：`[文件數量, 嵌入維度]`\n",
    "\n",
    "3. **`np.dot(文件矩陣, 查詢向量)`**\n",
    "   - 計算文件矩陣和查詢向量的點積\n",
    "   - 等同於計算每個文件向量與查詢向量的餘弦相似度\n",
    "   - 因為向量已經經過 L2 正規化，點積結果就等於餘弦相似度\n",
    "\n",
    "### 舉例說明：\n",
    "如果有：\n",
    "- 2 個文件\n",
    "- 嵌入維度是 3\n",
    "- 一個查詢向量\n",
    "\n",
    "則運算過程為：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa4c08",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "文件矩陣: [[0.1, 0.2, 0.3],    點積    查詢向量: [0.4, 0.5, 0.6]\n",
    "          [0.7, 0.8, 0.9]]     ==>    相似度: [0.32, 0.77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95213e9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 輸出結果：\n",
    "- 返回一個一維數組，長度等於文件數量\n",
    "- 每個數值代表對應文件與查詢的相似度分數\n",
    "- 分數越高表示越相關"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

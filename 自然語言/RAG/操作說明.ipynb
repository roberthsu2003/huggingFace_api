{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberthsu2003/huggingFace_api/blob/main/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80/RAG/%E6%93%8D%E4%BD%9C%E8%AA%AA%E6%98%8E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28735089",
      "metadata": {
        "id": "28735089"
      },
      "source": [
        "### 檢索增強生成\n",
        "- 使用模型(使用Microsoft開源的intfloat/multilingual-e5-large)\n",
        "- 模型大約要2.24GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910e43c6",
      "metadata": {
        "id": "910e43c6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "#模型名稱\n",
        "model_name = \"intfloat/multilingual-e5-large\"\n",
        "\n",
        "#載入tokenizer和模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) #\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5f957a",
      "metadata": {
        "id": "5e5f957a"
      },
      "source": [
        "# XLMRobertaTokenizerFast 介紹\n",
        "\n",
        "XLMRobertaTokenizerFast 是一個優化版的多語言分詞器(tokenizer)，主要具有以下特點：\n",
        "\n",
        "### 核心功能\n",
        "- 支援多語言處理能力\n",
        "- 採用 SentencePiece 演算法進行分詞\n",
        "- 使用 Rust 實現的快速版本，比原始 Python 版本效能更好\n",
        "\n",
        "### 重要參數說明\n",
        "1. **vocab_size**: 250002\n",
        "   - 詞彙表大小，包含所有可能的 token\n",
        "\n",
        "2. **model_max_length**: 512\n",
        "   - 模型可處理的最大序列長度\n",
        "\n",
        "3. **special_tokens**:\n",
        "   - `<s>`: 句子開始標記\n",
        "   - `</s>`: 句子結束標記\n",
        "   - `<unk>`: 未知詞標記\n",
        "   - `<pad>`: 填充標記\n",
        "   - `<mask>`: 遮蔽標記(用於遮蔽語言模型預訓練)\n",
        "\n",
        "### 使用優勢\n",
        "1. **快速處理**:\n",
        "   - Fast 版本比標準版本處理速度快很多\n",
        "   - 特別適合處理大量文本數據\n",
        "\n",
        "2. **多語言支援**:\n",
        "   - 可以處理多種語言的文本\n",
        "   - 不需要為不同語言使用不同的分詞器\n",
        "\n",
        "3. **與 XLM-RoBERTa 模型完美整合**:\n",
        "   - 專門為 XLM-RoBERTa 模型設計\n",
        "   - 確保輸入數據格式正確\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5dd79e0",
      "metadata": {
        "id": "b5dd79e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pprint import pprint\n",
        "#輸入文本,XLMRobertaTokenizerFast,要求的是list的型別\n",
        "#中文和英文可以比較2個語意相似度\n",
        "texts = [\"This is a test sentence.\", \"這是一個測試句子.\"]\n",
        "\n",
        "#將texts大內容編碼成為XLM-RoBERTa接受輸入的格式\n",
        "#padding是讓第2維的元素數量相同\n",
        "#return_tensors,讓input_ids的value是pytorch的tensor\n",
        "tokenized = tokenizer(texts,truncation=True,padding=True, return_tensors='pt')\n",
        "pprint(tokenized,compact=True) #類似dict類型的資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "249ec9ff",
      "metadata": {
        "id": "249ec9ff"
      },
      "outputs": [],
      "source": [
        "#載入模型\n",
        "model = AutoModel.from_pretrained(model_name) #2.24GB\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7910fced",
      "metadata": {
        "id": "7910fced"
      },
      "outputs": [],
      "source": [
        "#使用模型生成輸出的嵌入(embeded)\n",
        "model.eval() # 設定為評估模式\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized)\n",
        "outputs #我們要的資料儲存於last_hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07d4bbd",
      "metadata": {
        "id": "f07d4bbd"
      },
      "source": [
        "# `torch.no_grad()` 的用途說明\n",
        "\n",
        "### 主要目的\n",
        "`torch.no_grad()` 是一個上下文管理器 (context manager)，主要有以下用途：\n",
        "\n",
        "1. **停用梯度計算**\n",
        "   - 在推論 (inference) 階段不需要計算梯度\n",
        "   - 減少記憶體使用量\n",
        "   - 加快運算速度\n",
        "\n",
        "2. **記憶體優化**\n",
        "   - 不會儲存計算圖 (computational graph)\n",
        "   - 不會為反向傳播 (backpropagation) 保存中間結果\n",
        "\n",
        "### 使用時機\n",
        "\n",
        "1. **模型評估階段**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a57439",
      "metadata": {
        "id": "a9a57439"
      },
      "outputs": [],
      "source": [
        "model.eval()  # 設定為評估模式\n",
        "with torch.no_grad():\n",
        "    predictions = model(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd52d5df",
      "metadata": {
        "id": "fd52d5df"
      },
      "source": [
        "\n",
        "\n",
        "2. **推論階段**\n",
        "   - 當我們只需要模型的輸出結果\n",
        "   - 不需要進行模型訓練時\n",
        "\n",
        "### 效能比較\n",
        "\n",
        "使用 `torch.no_grad()` 的好處：\n",
        "- 記憶體使用量可減少約一半\n",
        "- 計算速度提升約20%-30%\n",
        "- 特別適合處理大型模型或大量數據\n",
        "\n",
        "### 注意事項\n",
        "\n",
        "1. **訓練時不要使用**\n",
        "   - 訓練階段需要梯度計算\n",
        "   - 使用 `no_grad()` 會影響模型學習\n",
        "\n",
        "2. **使用範圍**\n",
        "   - 只在需要推論的代碼區塊使用\n",
        "   - 可以和 `model.eval()` 配合使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c402b6e2",
      "metadata": {
        "id": "c402b6e2"
      },
      "outputs": [],
      "source": [
        "#提取嵌入結果\n",
        "embeddings = outputs.last_hidden_state #embeddings是一個tensor\n",
        "print(\"嵌入向量的形狀:\",embeddings.shape) #3維資料,2筆數據,每一筆有16個序列,每一個分詞有1024個資料 [批次大小, 序列長度, 隱藏層大小]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914076c4",
      "metadata": {
        "id": "914076c4"
      },
      "outputs": [],
      "source": [
        "#提取嵌入平均\n",
        "sentence_embeddings = embeddings.mean(dim=1)\n",
        "sentence_embeddings\n",
        "print(f\"sentence_embeddings的形狀:{sentence_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188a9de7",
      "metadata": {
        "id": "188a9de7"
      },
      "source": [
        "# 向量平均的計算說明\n",
        "\n",
        "這段程式碼在計算句子的最終向量表示，讓我們拆解來看：\n",
        "\n",
        "### 程式碼解析\n",
        "\n",
        "1. **embeddings 的結構**:\n",
        "   - 是一個 3 維張量 (tensor)\n",
        "   - 維度分別代表：`[批次大小, 序列長度, 隱藏層大小]`\n",
        "   - 在這個例子中可能是：`[2, 16, 1024]`\n",
        "     - 2：處理 2 個句子\n",
        "     - 16：每個句子被分成 16 個 token\n",
        "     - 1024：每個 token 的向量維度\n",
        "\n",
        "2. **mean(dim=1) 的作用**:\n",
        "   - `dim=1` 表示在第二個維度（序列長度）上取平均\n",
        "   - 將每個句子的所有 token 向量平均起來\n",
        "   - 結果會得到 `[2, 1024]` 的張量\n",
        "     - 2：仍然是 2 個句子\n",
        "     - 1024：每個句子的最終向量表示\n",
        "\n",
        "### 實際例子\n",
        "假設我們有以下結構：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e602435",
      "metadata": {
        "id": "2e602435"
      },
      "outputs": [],
      "source": [
        "# 原始 embeddings 結構\n",
        "[\n",
        "    # 第一個句子的 16 個 token 向量\n",
        "    [[0.1, 0.2, ...], [0.3, 0.4, ...], ...],\n",
        "    # 第二個句子的 16 個 token 向量\n",
        "    [[0.5, 0.6, ...], [0.7, 0.8, ...], ...]\n",
        "]\n",
        "\n",
        "# 取平均後的 sentence_embeddings\n",
        "[\n",
        "    [0.25, 0.35, ...],  # 第一個句子的平均向量\n",
        "    [0.65, 0.75, ...]   # 第二個句子的平均向量\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fdad9a",
      "metadata": {
        "id": "e7fdad9a"
      },
      "source": [
        "\n",
        "\n",
        "這種平均操作可以將不同長度的句子轉換成相同維度的向量表示，便於後續的相似度比較或其他處理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4073a004",
      "metadata": {
        "id": "4073a004"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# 計算餘弦相似度\n",
        "similarity = cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0))\n",
        "similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd492519",
      "metadata": {
        "id": "dd492519"
      },
      "source": [
        "# Tensor 維度擴展 - unsqueeze 說明\n",
        "\n",
        "### unsqueeze(0) 的功能\n",
        "`unsqueeze(0)` 是 PyTorch 中用於在指定位置增加一個維度的方法。數字 `0` 表示在第一個維度（最外層）增加一個維度。\n",
        "\n",
        "### 實際範例解析\n",
        "\n",
        "假設原始 tensor 形狀為：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c28f282",
      "metadata": {
        "id": "1c28f282"
      },
      "outputs": [],
      "source": [
        "sentence_embeddings[0]  # shape: [1024]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b24530f",
      "metadata": {
        "id": "1b24530f"
      },
      "source": [
        "\n",
        "\n",
        "使用 unsqueeze(0) 後：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2344a8c6",
      "metadata": {
        "id": "2344a8c6"
      },
      "outputs": [],
      "source": [
        "sentence_embeddings[0].unsqueeze(0)  # shape: [1, 1024]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72d6ef7",
      "metadata": {
        "id": "d72d6ef7"
      },
      "source": [
        "\n",
        "\n",
        "### 為什麼需要 unsqueeze?\n",
        "\n",
        "1. **批次處理需求**\n",
        "   - 很多 PyTorch 函數要求輸入必須是批次形式\n",
        "   - 單個樣本需要轉換為 [1, features] 的形狀\n",
        "\n",
        "2. **計算相似度時的維度匹配**\n",
        "   - 在計算餘弦相似度時常需要這種形狀\n",
        "   - 確保維度對齊以進行正確的矩陣運算\n",
        "\n",
        "### 程式碼示例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd01b0ec",
      "metadata": {
        "id": "fd01b0ec"
      },
      "outputs": [],
      "source": [
        "# 原始向量\n",
        "vector = torch.tensor([1, 2, 3])  # shape: [3]\n",
        "\n",
        "# 增加維度\n",
        "expanded = vector.unsqueeze(0)    # shape: [1, 3]\n",
        "\n",
        "# 用於計算相似度\n",
        "similarity = torch.cosine_similarity(expanded, other_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6bebc7d",
      "metadata": {
        "id": "a6bebc7d"
      },
      "source": [
        "\n",
        "\n",
        "### 常見使用場景\n",
        "- 模型推論時的單樣本處理\n",
        "- 向量相似度計算\n",
        "- 批次處理操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e62fed",
      "metadata": {
        "id": "f5e62fed"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
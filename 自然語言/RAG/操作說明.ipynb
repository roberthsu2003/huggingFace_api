{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28735089",
   "metadata": {},
   "source": [
    "### 檢索增強生成\n",
    "- 使用dataset(roberthsu2003/data_for_RAG)\n",
    "- 使用模型(使用Microsoft開源的intfloat/multilingual-e5-large)\n",
    "- 模型大約要2.24GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e43c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60983586dfa94456abf4bb132d58d986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42a156326f746fab8b21889fca8024e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a9c5875499427290870118d42ba73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50edf6323cfe456db251b3791175d555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='intfloat/multilingual-e5-large', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#模型名稱\n",
    "model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "#載入tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f957a",
   "metadata": {},
   "source": [
    "# XLMRobertaTokenizerFast 介紹\n",
    "\n",
    "XLMRobertaTokenizerFast 是一個優化版的多語言分詞器(tokenizer)，主要具有以下特點：\n",
    "\n",
    "### 核心功能\n",
    "- 支援多語言處理能力\n",
    "- 採用 SentencePiece 演算法進行分詞\n",
    "- 使用 Rust 實現的快速版本，比原始 Python 版本效能更好\n",
    "\n",
    "### 重要參數說明\n",
    "1. **vocab_size**: 250002\n",
    "   - 詞彙表大小，包含所有可能的 token\n",
    "\n",
    "2. **model_max_length**: 512 \n",
    "   - 模型可處理的最大序列長度\n",
    "\n",
    "3. **special_tokens**:\n",
    "   - `<s>`: 句子開始標記\n",
    "   - `</s>`: 句子結束標記\n",
    "   - `<unk>`: 未知詞標記\n",
    "   - `<pad>`: 填充標記\n",
    "   - `<mask>`: 遮蔽標記(用於遮蔽語言模型預訓練)\n",
    "\n",
    "### 使用優勢\n",
    "1. **快速處理**:\n",
    "   - Fast 版本比標準版本處理速度快很多\n",
    "   - 特別適合處理大量文本數據\n",
    "\n",
    "2. **多語言支援**:\n",
    "   - 可以處理多種語言的文本\n",
    "   - 不需要為不同語言使用不同的分詞器\n",
    "\n",
    "3. **與 XLM-RoBERTa 模型完美整合**:\n",
    "   - 專門為 XLM-RoBERTa 模型設計\n",
    "   - 確保輸入數據格式正確\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5dd79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[     0,   3293,     83,     10,   3034, 149357,      5,      2],\n",
      "        [     0,      6, 215617,  64770,  27683,   1344,      5,      2]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "#輸入文本,XLMRobertaTokenizerFast,要求的是list的型別\n",
    "#中文和英文可以比較2個語意相似度\n",
    "texts = [\"This is a test sentence.\", \"這是一個測試句子.\"]\n",
    "\n",
    "#將texts大內容編碼成為XLM-RoBERTa接受輸入的格式\n",
    "#padding是讓第2維的元素數量相同\n",
    "#return_tensors,讓input_ids的value是pytorch的tensor\n",
    "tokenized = tokenizer(texts,truncation=True,padding=True, return_tensors='pt')\n",
    "pprint(tokenized,compact=True) #類似dict類型的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "249ec9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#載入模型\n",
    "model = AutoModel.from_pretrained(model_name) #2.24GB\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7910fced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1858, -0.0582, -0.8848,  ..., -0.2282, -0.8866,  0.8900],\n",
       "         [ 0.2669, -0.5489, -0.7648,  ...,  0.0717, -0.6237,  0.7811],\n",
       "         [-0.0197, -0.4468, -0.8202,  ..., -0.5010, -0.7877,  0.7135],\n",
       "         ...,\n",
       "         [-0.7589, -0.2531, -0.6186,  ..., -0.7020, -0.8006,  0.8468],\n",
       "         [-0.1006, -0.1541, -0.6499,  ..., -0.4303, -0.9476,  0.6015],\n",
       "         [ 0.1857, -0.0580, -0.8845,  ..., -0.2282, -0.8864,  0.8901]],\n",
       "\n",
       "        [[ 1.3186, -0.1264, -0.6599,  ..., -0.5767, -0.6413,  0.7367],\n",
       "         [ 1.2949, -0.2761, -0.7651,  ..., -0.7754, -0.6019,  0.5162],\n",
       "         [ 1.3392, -0.4793, -0.5624,  ..., -0.5856, -0.5774,  0.4940],\n",
       "         ...,\n",
       "         [ 0.9948, -0.2463, -0.7340,  ..., -0.7638, -0.4298,  0.4737],\n",
       "         [ 0.6910, -0.1470, -0.4917,  ..., -0.6273, -0.8047,  0.4675],\n",
       "         [ 1.3187, -0.1263, -0.6597,  ..., -0.5768, -0.6413,  0.7367]]]), pooler_output=tensor([[-0.7886,  0.5136, -0.0641,  ...,  0.2522, -0.5041,  0.2640],\n",
       "        [-0.7112,  0.5049, -0.6078,  ...,  0.2556, -0.3396,  0.0432]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用模型生成輸出的嵌入(embeded)\n",
    "model.eval() # 設定為評估模式\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized)\n",
    "outputs #我們要的資料儲存於last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d4bbd",
   "metadata": {},
   "source": [
    "# `torch.no_grad()` 的用途說明\n",
    "\n",
    "### 主要目的\n",
    "`torch.no_grad()` 是一個上下文管理器 (context manager)，主要有以下用途：\n",
    "\n",
    "1. **停用梯度計算**\n",
    "   - 在推論 (inference) 階段不需要計算梯度\n",
    "   - 減少記憶體使用量\n",
    "   - 加快運算速度\n",
    "\n",
    "2. **記憶體優化**\n",
    "   - 不會儲存計算圖 (computational graph)\n",
    "   - 不會為反向傳播 (backpropagation) 保存中間結果\n",
    "\n",
    "### 使用時機\n",
    "\n",
    "1. **模型評估階段**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a57439",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # 設定為評估模式\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52d5df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. **推論階段**\n",
    "   - 當我們只需要模型的輸出結果\n",
    "   - 不需要進行模型訓練時\n",
    "\n",
    "### 效能比較\n",
    "\n",
    "使用 `torch.no_grad()` 的好處：\n",
    "- 記憶體使用量可減少約一半\n",
    "- 計算速度提升約20%-30%\n",
    "- 特別適合處理大型模型或大量數據\n",
    "\n",
    "### 注意事項\n",
    "\n",
    "1. **訓練時不要使用**\n",
    "   - 訓練階段需要梯度計算\n",
    "   - 使用 `no_grad()` 會影響模型學習\n",
    "\n",
    "2. **使用範圍**\n",
    "   - 只在需要推論的代碼區塊使用\n",
    "   - 可以和 `model.eval()` 配合使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入向量的形狀: torch.Size([2, 16, 1024])\n"
     ]
    }
   ],
   "source": [
    "#提取嵌入結果\n",
    "embeddings = outputs.last_hidden_state #embeddings是一個tensor\n",
    "print(\"嵌入向量的形狀:\",embeddings.shape) #3維資料,2筆數據,每一筆有16個序列,每一個分詞有1024個資料 [批次大小, 序列長度, 隱藏層大小]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "914076c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_embeddings的形狀:torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "#提取嵌入平均\n",
    "sentence_embeddings = embeddings.mean(dim=1)\n",
    "sentence_embeddings\n",
    "print(f\"sentence_embeddings的形狀:{sentence_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a9de7",
   "metadata": {},
   "source": [
    "# 向量平均的計算說明\n",
    "\n",
    "這段程式碼在計算句子的最終向量表示，讓我們拆解來看：\n",
    "\n",
    "### 程式碼解析\n",
    "\n",
    "1. **embeddings 的結構**:\n",
    "   - 是一個 3 維張量 (tensor)\n",
    "   - 維度分別代表：`[批次大小, 序列長度, 隱藏層大小]`\n",
    "   - 在這個例子中可能是：`[2, 16, 1024]`\n",
    "     - 2：處理 2 個句子\n",
    "     - 16：每個句子被分成 16 個 token\n",
    "     - 1024：每個 token 的向量維度\n",
    "\n",
    "2. **mean(dim=1) 的作用**:\n",
    "   - `dim=1` 表示在第二個維度（序列長度）上取平均\n",
    "   - 將每個句子的所有 token 向量平均起來\n",
    "   - 結果會得到 `[2, 1024]` 的張量\n",
    "     - 2：仍然是 2 個句子\n",
    "     - 1024：每個句子的最終向量表示\n",
    "\n",
    "### 實際例子\n",
    "假設我們有以下結構：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e602435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始 embeddings 結構\n",
    "[\n",
    "    # 第一個句子的 16 個 token 向量\n",
    "    [[0.1, 0.2, ...], [0.3, 0.4, ...], ...], \n",
    "    # 第二個句子的 16 個 token 向量\n",
    "    [[0.5, 0.6, ...], [0.7, 0.8, ...], ...]\n",
    "]\n",
    "\n",
    "# 取平均後的 sentence_embeddings\n",
    "[\n",
    "    [0.25, 0.35, ...],  # 第一個句子的平均向量\n",
    "    [0.65, 0.75, ...]   # 第二個句子的平均向量\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdad9a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "這種平均操作可以將不同長度的句子轉換成相同維度的向量表示，便於後續的相似度比較或其他處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4073a004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87943894]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# 計算餘弦相似度\n",
    "similarity = cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd492519",
   "metadata": {},
   "source": [
    "# Tensor 維度擴展 - unsqueeze 說明\n",
    "\n",
    "### unsqueeze(0) 的功能\n",
    "`unsqueeze(0)` 是 PyTorch 中用於在指定位置增加一個維度的方法。數字 `0` 表示在第一個維度（最外層）增加一個維度。\n",
    "\n",
    "### 實際範例解析\n",
    "\n",
    "假設原始 tensor 形狀為：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings[0]  # shape: [1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24530f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "使用 unsqueeze(0) 後：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings[0].unsqueeze(0)  # shape: [1, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d6ef7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 為什麼需要 unsqueeze?\n",
    "\n",
    "1. **批次處理需求**\n",
    "   - 很多 PyTorch 函數要求輸入必須是批次形式\n",
    "   - 單個樣本需要轉換為 [1, features] 的形狀\n",
    "\n",
    "2. **計算相似度時的維度匹配**\n",
    "   - 在計算餘弦相似度時常需要這種形狀\n",
    "   - 確保維度對齊以進行正確的矩陣運算\n",
    "\n",
    "### 程式碼示例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始向量\n",
    "vector = torch.tensor([1, 2, 3])  # shape: [3]\n",
    "\n",
    "# 增加維度\n",
    "expanded = vector.unsqueeze(0)    # shape: [1, 3]\n",
    "\n",
    "# 用於計算相似度\n",
    "similarity = torch.cosine_similarity(expanded, other_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bebc7d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 常見使用場景\n",
    "- 模型推論時的單樣本處理\n",
    "- 向量相似度計算\n",
    "- 批次處理操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e62fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

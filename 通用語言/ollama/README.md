## ä½¿ç”¨ollamaå’ŒopenWebUI

1. [å®‰è£Docker](#å®‰è£Docker)
2. [æœ¬æ©Ÿå®‰è£Ollama](#æœ¬æ©Ÿå®‰è£Ollama)
3. [dockerå®‰è£Ollama](#dockerå®‰è£Ollama)
4. [dockerå®‰è£OpenWebUI](#dockerå®‰è£OpenWebUI)
5. [requestsé€£çµollama](#requestsé€£çµollama)
8. [é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api](#é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api)

<a name="å®‰è£Docker"></a>
### 1. å®‰è£Docker
- https://docs.docker.com/get-started/get-docker/
---
<a name="æœ¬æ©Ÿå®‰è£Ollama"></a>
### 2. æœ¬æ©Ÿå®‰è£Ollama
- å„ªé»:æœƒç›´æ¥ä½¿ç”¨Macçš„GPU,ç„¡GPUè‡ªå‹•ä½¿ç”¨CPU
- [ollamaå®˜ç¶²](https://ollama.com)
- ollamaæ”¯æ´çš„æ¨¡å‹,è«‹åƒè€ƒollamçš„å®˜ç¶²

**2.1 OllamaåŸºæœ¬æŒ‡ä»¤**

**ä¸‹è¼‰æ¨¡å‹**

- macé è¨­ä¸‹è¼‰çš„ç›®éŒ„`~/.ollama/models/blobs`  
- windows `C:\Users\<ç”¨æˆ·å>\.ollama\models`  
- linux `~/.ollama/models`  
- æ›´æ”¹é è¨­ç›®éŒ„è¦ä¿®æ”¹ç’°å¢ƒè®Šæ•¸`OLLAMA_MODELS` ä¾†æŒ‡å®šæ–°çš„å„²å­˜è·¯å¾‘

**æ›´æ”¹ä¸‹è¼‰ç›®éŒ„**

- **windows**

```
Windows
	1.	ä¿®æ”¹ç³»çµ±ç’°å¢ƒè®Šæ•¸ï¼š
	â€¢	å³éµé»æ“Šã€Œæ­¤é›»è…¦ã€ï¼Œé¸æ“‡ã€Œå±¬æ€§ã€ã€‚
	â€¢	é»æ“Šã€Œé«˜ç´šç³»çµ±è¨­å®šã€ã€‚
	â€¢	åœ¨ã€Œç³»çµ±å±¬æ€§ã€çª—å£ä¸­ï¼Œé¸æ“‡ã€Œé«˜ç´šã€é¸é …å¡ï¼Œç„¶å¾Œé»æ“Šã€Œç’°å¢ƒè®Šæ•¸ã€ã€‚
	â€¢	åœ¨ã€Œç³»çµ±è®Šæ•¸ã€éƒ¨åˆ†ï¼Œé»æ“Šã€Œæ–°å»ºã€ã€‚
	â€¢	è¼¸å…¥ã€Œè®Šé‡åã€ç‚º `OLLAMA_MODELS`ï¼Œä¸¦è¨­å®šã€Œè®Šé‡å€¼ã€ç‚ºæ‚¨æƒ³è¦çš„è‡ªå®šç¾©è·¯å¾‘ï¼ˆä¾‹å¦‚ `D:\Ollama\Models`ï¼‰ã€‚
	â€¢	é»æ“Šã€Œç¢ºå®šã€ä¿å­˜è®Šæ›´ã€‚
	2.	é‡å•Ÿ Ollamaï¼š
	â€¢	é€€å‡º Ollama æœå‹™ï¼ˆå³éµé»æ“Š Ollama åœ–æ¨™ï¼Œé¸æ“‡ã€Œé€€å‡ºã€ï¼‰ã€‚
	â€¢	é‡æ–°å•Ÿå‹• Ollama æœå‹™ã€‚
```

- **mac**

```
	1.	è¨­å®šç’°å¢ƒè®Šæ•¸ï¼š
	â€¢	æ‰“é–‹çµ‚ç«¯æ©Ÿã€‚
	â€¢	å¦‚æœæ‚¨ä½¿ç”¨ Bashï¼Œç·¨è¼¯ `~/.bashrc` æ–‡ä»¶ï¼š
	nano ~/.bashrc
	
	å¦‚æœæ‚¨ä½¿ç”¨ Zshï¼Œç·¨è¼¯ `~/.zshrc` æ–‡ä»¶ï¼š
	nano ~/.zshrc
	
	åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ä»¥ä¸‹è¡Œï¼š
	export OLLAMA_MODELS=~/ollama_download_models
	
	é‡æ–°è¼‰å…¥é…ç½®ï¼š
	source ~/.bashrc
	source ~/.zshrc
	
	é‡æ–°é–‹æ©Ÿ
  
```

**æª¢æŸ¥æ˜¯å¦å·²ç¶“å®‰è£Ollama**

```bash
ollama --version
```

**æŸ¥è©¢å¯ä»¥ä¸‹è¼‰çš„æ¨¡å‹**

[æŸ¥è©¢Ollamaæ”¯æ´çš„Model](https://ollama.com/search)

**ä¸‹è¼‰æ¨¡å‹æŒ‡ä»¤**

```bash
ollama pull llama3.2:3b
```

**æª¢è¦–ç›®å‰å·²ç¶“ä¸‹è¼‰çš„æ¨¡å‹**

```bash
ollama list
```

**åŸ·è¡Œæ¨¡å‹**

```bash
ollama run llama3.2
```

**åœæ­¢åŸ·è¡Œæ¨¡å‹**

```
>>> /bye
```


**ç›®å‰è¢«è¼‰å…¥çš„æ¨¡å‹**
- å¯ä»¥æŸ¥çœ‹æ˜¯å¦æ˜¯ä½¿ç”¨GPU

```bash
ollama ps

#=====output===
NAME         ID              SIZE      PROCESSOR    UNTIL              
gemma3:4b    a2af6cc3eb7f    6.3 GB    100% GPU     3 minutes from now
```

**åœæ­¢æ¨¡å‹**

```bash
ollama stop llama3.2
```

**åˆªé™¤æ¨¡å‹**

```bash
ollama rm llama3.2
```

---


<a name="dockerå®‰è£Ollama"></a>
### 3. dockerå®‰è£Ollama
- ç¼ºé»:éƒ½æ˜¯ä½¿ç”¨CPU
- å¦‚æœè¦æ”¯æ´NVIDIA,è«‹å®‰è£NVIDIA Container Toolkitâ .

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```


> [!IMPORTANT]
> å®‰è£å®Œå¾Œ,å¯ä»¥åˆ©ç”¨docker desktopå…§container Execä¾†åŸ·è¡Œollamaçš„æŒ‡ä»¤

---

<a name="dockerå®‰è£OpenWebUI"></a>
### 4. dockerå®‰è£OpenWebUI

- å¯ä»¥ä½¿ç”¨é€éç¶²è·¯é€£ç·šçš„å…è²»æ¨¡å‹(Chatbot Arenaæœ‰17bçš„åƒæ•¸é‡)
- è¨­å®š->ç®¡ç†å“¡è¨­å®š->é€£ç·š->é–‹å•Ÿç›´æ¥é€£ç·š(æœƒè‡ªå‹•é€£ç·šChatbot Arena)

- ç”±æ–¼ä½¿ç”¨äº†restart always,dockerä¸€è¢«å•Ÿå‹•å°±æœƒè‡ªå‹•é–‹å•Ÿcontainer(æ¯”è¼ƒç§è³‡æº)

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```


- æ‰‹å‹•é–‹å•Ÿcontainerçš„èªæ³•

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui ghcr.io/open-webui/open-webui:main
```

**ä½¿ç”¨ç€è¦½å™¨å•Ÿå‹•http://localhost:8080**

---
<a name=â€œrequestsé€£çµollamaâ€></a>

### 5. requestsé€£çµollama

```python
import requests

def chat_with_ollama(prompt: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "gemma3:1b",
        "prompt": prompt,
        "stream": False,
        "options": { #åƒè€ƒèªªæ˜1
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        },
        "max_tokens": 100,
        "format": "json",
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ğŸ’¬ AI å›æ‡‰ï¼š")
    # Print the whole result for debugging
    print(result)
    # Try to print the 'response' key if it exists, otherwise print possible keys
    if "response" in result:
        print(result["response"])
    elif "message" in result:
        print(result["message"])
    elif "content" in result:
        print(result["content"])
    else:
        print("No expected key found in response. Available keys:", result.keys())

#ç¯„ä¾‹è¼¸å…¥
chat_with_ollama("è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ")
```

<a name="é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api"></a>
### 8. é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api

![](./images/pic1.png)

- [ollama apiå®˜æ–¹èªªæ˜æ›¸](https://github.com/ollama/ollama/blob/main/docs/api.md)

> [!IMPORTANT]
> 1. ollamaçš„æœæ•¸è¦é–‹å•Ÿ
> 2. ollama run æ¨¡å‹åç¨±
> 3. æœ¬åœ°ç«¯æˆ–docker apiç¶²å€ä¸ä¸€æ¨£

- [**å¯¦ä½œpy**](./interface.py)


```python
import requests
import json
import gradio as gr

# å‘¼å« Ollama çš„å‡½å¼
def ask_ollama(prompt):
    #url = "http://localhost:11434/api/generate"
    url = "http://host.docker.internal:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": "llama3.2:3b",
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            return response.json()['response']
        else:
            return f"âš ï¸ Error: {response.status_code} - {response.text}"
    except Exception as e:
        return f"âŒ Exception: {str(e)}"

# å»ºç«‹ Gradio ä»‹é¢
iface = gr.Interface(
    fn=ask_ollama,
    inputs=gr.Textbox(label="è¼¸å…¥ä½ çš„å•é¡Œ"),
    outputs=gr.Textbox(label="æ¨¡å‹çš„å›ç­”"),
    title="ğŸ¦™ Ollama èŠå¤©ä»‹é¢ (LLaMA3)",
    description="è¼¸å…¥æ–‡å­—ï¼Œè®“æœ¬æ©Ÿçš„ LLaMA3 æ¨¡å‹å›ç­”ä½ "
)

# å•Ÿå‹•ä»‹é¢
iface.launch()
```

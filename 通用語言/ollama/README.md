## ä½¿ç”¨ollamaå’ŒopenWebUI

1. [å®‰è£Docker](#å®‰è£Docker)
2. [æœ¬æ©Ÿå®‰è£Ollama](#æœ¬æ©Ÿå®‰è£Ollama)
3. [dockerå®‰è£Ollama](#dockerå®‰è£Ollama)
4. [dockerå®‰è£OpenWebUI](#dockerå®‰è£OpenWebUI)
5. [requestsé€£çµollama](#requestsé€£çµollama)
6. [requestsé€£çµollama_generate_mode](#requestsé€£çµollama_generate_mode)
7. [requestsé€£çµchat_mode](#requestsé€£çµchat_mode)
8. [é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api](#é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api)

<a name="å®‰è£Docker"></a>
### 1. å®‰è£Docker
- https://docs.docker.com/get-started/get-docker/
---
<a name="æœ¬æ©Ÿå®‰è£Ollama"></a>
### 2. æœ¬æ©Ÿå®‰è£Ollama
- å„ªé»:æœƒç›´æ¥ä½¿ç”¨Macçš„GPU,ç„¡GPUè‡ªå‹•ä½¿ç”¨CPU
- [ollamaå®˜ç¶²](https://ollama.com)
- ollamaæ”¯æ´çš„æ¨¡å‹,è«‹åƒè€ƒollamçš„å®˜ç¶²

**2.1 OllamaåŸºæœ¬æŒ‡ä»¤**

**ä¸‹è¼‰æ¨¡å‹**

- macé è¨­ä¸‹è¼‰çš„ç›®éŒ„`~/.ollama/models/blobs`  
- windows `C:\Users\<ç”¨æˆ·å>\.ollama\models`  
- linux `~/.ollama/models`  
- æ›´æ”¹é è¨­ç›®éŒ„è¦ä¿®æ”¹ç’°å¢ƒè®Šæ•¸`OLLAMA_MODELS` ä¾†æŒ‡å®šæ–°çš„å„²å­˜è·¯å¾‘

**æ›´æ”¹ä¸‹è¼‰ç›®éŒ„**

- **windows**

```
Windows
	1.	ä¿®æ”¹ç³»çµ±ç’°å¢ƒè®Šæ•¸ï¼š
	â€¢	å³éµé»æ“Šã€Œæ­¤é›»è…¦ã€ï¼Œé¸æ“‡ã€Œå±¬æ€§ã€ã€‚
	â€¢	é»æ“Šã€Œé«˜ç´šç³»çµ±è¨­å®šã€ã€‚
	â€¢	åœ¨ã€Œç³»çµ±å±¬æ€§ã€çª—å£ä¸­ï¼Œé¸æ“‡ã€Œé«˜ç´šã€é¸é …å¡ï¼Œç„¶å¾Œé»æ“Šã€Œç’°å¢ƒè®Šæ•¸ã€ã€‚
	â€¢	åœ¨ã€Œç³»çµ±è®Šæ•¸ã€éƒ¨åˆ†ï¼Œé»æ“Šã€Œæ–°å»ºã€ã€‚
	â€¢	è¼¸å…¥ã€Œè®Šé‡åã€ç‚º `OLLAMA_MODELS`ï¼Œä¸¦è¨­å®šã€Œè®Šé‡å€¼ã€ç‚ºæ‚¨æƒ³è¦çš„è‡ªå®šç¾©è·¯å¾‘ï¼ˆä¾‹å¦‚ `D:\Ollama\Models`ï¼‰ã€‚
	â€¢	é»æ“Šã€Œç¢ºå®šã€ä¿å­˜è®Šæ›´ã€‚
	2.	é‡å•Ÿ Ollamaï¼š
	â€¢	é€€å‡º Ollama æœå‹™ï¼ˆå³éµé»æ“Š Ollama åœ–æ¨™ï¼Œé¸æ“‡ã€Œé€€å‡ºã€ï¼‰ã€‚
	â€¢	é‡æ–°å•Ÿå‹• Ollama æœå‹™ã€‚
```

- **mac**

```
	1.	è¨­å®šç’°å¢ƒè®Šæ•¸ï¼š
	â€¢	æ‰“é–‹çµ‚ç«¯æ©Ÿã€‚
	â€¢	å¦‚æœæ‚¨ä½¿ç”¨ Bashï¼Œç·¨è¼¯ `~/.bashrc` æ–‡ä»¶ï¼š
	nano ~/.bashrc
	
	å¦‚æœæ‚¨ä½¿ç”¨ Zshï¼Œç·¨è¼¯ `~/.zshrc` æ–‡ä»¶ï¼š
	nano ~/.zshrc
	
	åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ä»¥ä¸‹è¡Œï¼š
	export OLLAMA_MODELS=~/ollama_download_models
	
	é‡æ–°è¼‰å…¥é…ç½®ï¼š
	source ~/.bashrc
	source ~/.zshrc
	
	é‡æ–°é–‹æ©Ÿ
  
```

**æª¢æŸ¥æ˜¯å¦å·²ç¶“å®‰è£Ollama**

```bash
ollama --version
```

**æŸ¥è©¢å¯ä»¥ä¸‹è¼‰çš„æ¨¡å‹**

[æŸ¥è©¢Ollamaæ”¯æ´çš„Model](https://ollama.com/search)

**ä¸‹è¼‰æ¨¡å‹æŒ‡ä»¤**

```bash
ollama pull llama3.2:3b
```

**æª¢è¦–ç›®å‰å·²ç¶“ä¸‹è¼‰çš„æ¨¡å‹**

```bash
ollama list
```

**åŸ·è¡Œæ¨¡å‹**

```bash
ollama run llama3.2
```

**åœæ­¢åŸ·è¡Œæ¨¡å‹**

```
>>> /bye
```


**ç›®å‰è¢«è¼‰å…¥çš„æ¨¡å‹**
- å¯ä»¥æŸ¥çœ‹æ˜¯å¦æ˜¯ä½¿ç”¨GPU

```bash
ollama ps

#=====output===
NAME         ID              SIZE      PROCESSOR    UNTIL              
gemma3:4b    a2af6cc3eb7f    6.3 GB    100% GPU     3 minutes from now
```

**åœæ­¢æ¨¡å‹**

```bash
ollama stop llama3.2
```

**åˆªé™¤æ¨¡å‹**

```bash
ollama rm llama3.2
```

---


<a name="dockerå®‰è£Ollama"></a>
### 3. dockerå®‰è£Ollama
- ç¼ºé»:éƒ½æ˜¯ä½¿ç”¨CPU
- å¦‚æœè¦æ”¯æ´NVIDIA,è«‹å®‰è£NVIDIA Container Toolkitâ .

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```


> [!IMPORTANT]
> å®‰è£å®Œå¾Œ,å¯ä»¥åˆ©ç”¨docker desktopå…§container Execä¾†åŸ·è¡Œollamaçš„æŒ‡ä»¤

---

<a name="dockerå®‰è£OpenWebUI"></a>
### 4. dockerå®‰è£OpenWebUI

- å¯ä»¥ä½¿ç”¨é€éç¶²è·¯é€£ç·šçš„å…è²»æ¨¡å‹(Chatbot Arenaæœ‰17bçš„åƒæ•¸é‡)
- è¨­å®š->ç®¡ç†å“¡è¨­å®š->é€£ç·š->é–‹å•Ÿç›´æ¥é€£ç·š(æœƒè‡ªå‹•é€£ç·šChatbot Arena)

- ç”±æ–¼ä½¿ç”¨äº†restart always,dockerä¸€è¢«å•Ÿå‹•å°±æœƒè‡ªå‹•é–‹å•Ÿcontainer(æ¯”è¼ƒç§è³‡æº)

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```


- æ‰‹å‹•é–‹å•Ÿcontainerçš„èªæ³•

```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui ghcr.io/open-webui/open-webui:main
```

**ä½¿ç”¨ç€è¦½å™¨å•Ÿå‹•http://localhost:8080**

---
<a name="requestsé€£çµollama"></a>
### 5. requestsé€£çµollama

```python
import requests

def chat_with_ollama(prompt: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "gemma3:1b",
        "prompt": prompt,
        "stream": False,
        "options": { #åƒè€ƒèªªæ˜1
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        },
        "max_tokens": 100,
        "format": "json",
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ğŸ’¬ AI å›æ‡‰ï¼š")
    # Print the whole result for debugging
    print(result)
    # Try to print the 'response' key if it exists, otherwise print possible keys
    if "response" in result:
        print(result["response"])
    elif "message" in result:
        print(result["message"])
    elif "content" in result:
        print(result["content"])
    else:
        print("No expected key found in response. Available keys:", result.keys())

#ç¯„ä¾‹è¼¸å…¥
chat_with_ollama("è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ")
```

### èªªæ˜1:

`options` ç‰©ä»¶å°è£äº†å°æ–¼ç”Ÿæˆæ¨¡å‹è¡Œç‚ºçš„ä¸‰å€‹é—œéµèª¿æ•´åƒæ•¸ï¼š`temperature`ã€`top_p` ä»¥åŠ `top_k`ã€‚é€éé€™äº›è¨­å®šï¼Œæˆ‘å€‘å¯ä»¥æ›´ç²¾ç´°åœ°æ§åˆ¶æ¨¡å‹åœ¨ç”¢ç”Ÿæ–‡å­—æ™‚çš„éš¨æ©Ÿç¨‹åº¦èˆ‡å¤šæ¨£æ€§ï¼Œä»¥é”åˆ°æ›´ç¬¦åˆéœ€æ±‚çš„è¼¸å‡ºé¢¨æ ¼ã€‚

`temperature`ï¼ˆæº«åº¦ï¼‰åƒæ•¸è¨­å®šç‚º 0.7ï¼Œè¡¨ç¤ºåœ¨æŒ‘é¸ä¸‹ä¸€å€‹å­—å…ƒæˆ–è©å½™æ™‚ï¼Œæœƒæ ¹æ“šæ¨¡å‹é æ¸¬æ©Ÿç‡åˆ†ä½ˆåšæº«åº¦ç¸®æ”¾ã€‚æº«åº¦è¶Šæ¥è¿‘ 1ï¼Œç”Ÿæˆçµæœè¶Šéš¨æ©Ÿã€å¤šæ¨£ï¼›ç•¶æº«åº¦é™ä½æ™‚ï¼Œç”Ÿæˆæ›´å‚¾å‘æ–¼é«˜æ©Ÿç‡é¸æ“‡ï¼Œè¼¸å‡ºçµæœè¼ƒç‚ºä¿å®ˆä¸”é‡è¤‡æ€§å¢åŠ ã€‚è¨­å®šç‚º 0.7 èƒ½åœ¨éš¨æ©Ÿæ€§èˆ‡ç©©å®šæ€§é–“å–å¾—å¹³è¡¡ã€‚

`top_p`ï¼ˆåˆç¨± nucleus samplingï¼‰è¨­ç‚º 0.9ï¼Œä»£è¡¨æ¯æ¬¡ç”Ÿæˆæ™‚åƒ…è€ƒæ…®ç´¯ç©æ©Ÿç‡å‰ 90% çš„å€™é¸è©å½™ã€‚æ›è¨€ä¹‹ï¼Œæ¨¡å‹å…ˆå°‡æ‰€æœ‰å€™é¸ä¾æ©Ÿç‡ç”±é«˜åˆ°ä½æ’åºï¼Œç„¶å¾Œå¾æ©Ÿç‡ç¸½å’Œé”åˆ° 0.9 çš„è©å½™å­é›†ä¸­é€²è¡Œéš¨æ©ŸæŠ½æ¨£ã€‚é€™ç¨®æ–¹æ³•å¯é¿å…åªé—œæ³¨æœ€é«˜æ©Ÿç‡è€Œå¿½ç•¥å…¶ä»–åˆç†é¸é …ï¼Œä¹Ÿèƒ½è‡ªå‹•èª¿æ•´æŠ½æ¨£ç¯„åœä»¥æŠ‘åˆ¶æ¥µä½æ©Ÿç‡çš„ã€Œå™ªéŸ³ã€è¼¸å‡ºã€‚

`top_k` åƒæ•¸è¨­ç½®ç‚º 50ï¼Œè¡¨ç¤ºåœ¨æŠ½æ¨£æ™‚åƒ…å¾é æ¸¬æ©Ÿç‡æœ€é«˜çš„å‰ 50 å€‹è©å½™ä¸­é¸æ“‡ä¸‹ä¸€æ­¥çµæœã€‚é€™æ˜¯åœ¨é™åˆ¶æœç´¢ç©ºé–“å¤§å°ã€æé«˜é‹ç®—æ•ˆç‡èˆ‡å“è³ªæ§åˆ¶çš„å¸¸è¦‹åšæ³•ã€‚çµåˆ `top_p` èˆ‡ `top_k` ä½¿ç”¨ï¼Œèƒ½é€²ä¸€æ­¥å¹³è¡¡å¤šæ¨£æ€§èˆ‡ç©©å®šæ€§ï¼š`top_k` ç¢ºä¿å€™é¸é›†ä¸è¶…éä¸€å®šè¦æ¨¡ï¼Œ`top_p` å‰‡ä¾å¯¦éš›æ©Ÿç‡åˆ†ä½ˆå‹•æ…‹ä¿®å‰ªé›†å…§è©å½™ã€‚

ç¶œåˆè€Œè¨€ï¼Œé€™ä¸‰é …åƒæ•¸å…±åŒç‚ºç”Ÿæˆæ¨¡å‹æä¾›äº†å¤šå±¤æ¬¡çš„éš¨æ©Ÿèˆ‡ç¯©é¸æ©Ÿåˆ¶ã€‚ä¾æ“šä¸åŒæ‡‰ç”¨å ´æ™¯ï¼ˆå¦‚å°è©±ç³»çµ±ã€æ–‡ç« æ’°å¯«æˆ–ç¨‹å¼ç¢¼ç”Ÿæˆï¼‰ï¼Œå¯å¾®èª¿é€™äº›å€¼ä»¥ç²å¾—æ›´ç¬¦åˆéœ€æ±‚çš„çµæœã€‚

<a name="requestsé€£çµollama_generate_mode"></a>
### 6. requestsé€£çµollama_generate_mode

```python
import requests
def chat_with_ollama(prompt: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "gemma3:1b",
        "prompt": prompt,
        "stream": False,
        "options": { #åƒè€ƒèªªæ˜1
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        },
        "max_tokens": 100,
        "format": "json",
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ğŸ’¬ AI å›æ‡‰ï¼š")
    # Print the whole result for debugging
    #print(result)
    # Try to print the 'response' key if it exists, otherwise print possible keys
    if "response" in result:
        print(result["response"])
    elif "message" in result:
        print(result["message"])
    elif "content" in result:
        print(result["content"])
    else:
        print("No expected key found in response. Available keys:", result.keys())

    
def chat_loop():
    print("æ­¡è¿ä½¿ç”¨æœ¬åœ°ç«¯ LLM èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰")
    while True:
        user_input = input("ğŸ‘¤ ä½ èªªï¼š")
        if user_input.lower() == 'q':
            break
        chat_with_ollama(user_input)

chat_loop()
```

---

<a name="requestsé€£çµchat_mode"></a>
### 7. requestsé€£çµchat_mode

```python
import requests
import json

def chat_with_ollama(prompt: str):
    url = "http://localhost:11434/api/chat"
    payload = {
        "model": "gemma3:1b",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "stream": True,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        }
    }

    print("ğŸ’¬ AI å›æ‡‰ï¼š", end="", flush=True)
    
    try:
        response = requests.post(url, json=payload, stream=True)
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line.decode('utf-8'))
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰è¨Šæ¯å…§å®¹
                    if 'message' in chunk and 'content' in chunk['message']:
                        content = chunk['message']['content']
                        print(content, end="", flush=True)
                    
                    # æª¢æŸ¥æ˜¯å¦å®Œæˆ
                    if chunk.get('done', False):
                        print()  # æ›è¡Œ
                        break
                        
                except json.JSONDecodeError:
                    continue
                    
    except requests.exceptions.RequestException as e:
        print(f"\nâŒ è«‹æ±‚éŒ¯èª¤: {e}")
    except Exception as e:
        print(f"\nâŒ è™•ç†éŒ¯èª¤: {e}")

def chat_loop():
    print("æ­¡è¿ä½¿ç”¨æœ¬åœ°ç«¯ LLM èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰")
    while True:
        user_input = input("ğŸ‘¤ ä½ èªªï¼š")
        if user_input.lower() == 'q':
            break
        chat_with_ollama(user_input)
        print()  # ç©ºè¡Œåˆ†éš”

chat_loop()
```

<a name="é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api"></a>
### 8. é€£æ¥gradioçš„ä»‹é¢å‘¼å«ollamaçš„api

![](./images/pic1.png)

- [ollama apiå®˜æ–¹èªªæ˜æ›¸](https://github.com/ollama/ollama/blob/main/docs/api.md)

> [!IMPORTANT]
> 1. ollamaçš„æœæ•¸è¦é–‹å•Ÿ
> 2. ollama run æ¨¡å‹åç¨±
> 3. æœ¬åœ°ç«¯æˆ–docker apiç¶²å€ä¸ä¸€æ¨£

- [**å¯¦ä½œpy**](./interface.py)


```python
import requests
import json
import gradio as gr

# å‘¼å« Ollama çš„å‡½å¼
def ask_ollama(prompt):
    #url = "http://localhost:11434/api/generate"
    url = "http://host.docker.internal:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": "llama3.2:3b",
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            return response.json()['response']
        else:
            return f"âš ï¸ Error: {response.status_code} - {response.text}"
    except Exception as e:
        return f"âŒ Exception: {str(e)}"

# å»ºç«‹ Gradio ä»‹é¢
iface = gr.Interface(
    fn=ask_ollama,
    inputs=gr.Textbox(label="è¼¸å…¥ä½ çš„å•é¡Œ"),
    outputs=gr.Textbox(label="æ¨¡å‹çš„å›ç­”"),
    title="ğŸ¦™ Ollama èŠå¤©ä»‹é¢ (LLaMA3)",
    description="è¼¸å…¥æ–‡å­—ï¼Œè®“æœ¬æ©Ÿçš„ LLaMA3 æ¨¡å‹å›ç­”ä½ "
)

# å•Ÿå‹•ä»‹é¢
iface.launch()
```
